{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YoonesVaezi(DataMLOp\\.conda\\envs\\t5\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import torch\n",
    "import datasets\n",
    "from datasets import load_from_disk, DatasetDict\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from evaluate import load\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from scipy.spatial.distance import cosine\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_model_name = 't5-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\YoonesVaezi(DataMLOp\\AppData\\Roaming\\nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\YoonesVaezi(DataMLOp\\AppData\\Roaming\\nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\YoonesVaezi(DataMLOp\\AppData\\Roaming\\nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\YoonesVaezi(DataMLOp\\AppData\\Roaming\\nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\YoonesVaezi(DataMLOp\\AppData\\Roaming\\nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\YoonesVaezi(DataMLOp\\AppData\\Roaming\\nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "meteor = load('meteor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "c:\\Users\\YoonesVaezi(DataMLOp\\.conda\\envs\\t5\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the stemmer for stemming words during text normalization\n",
    "stemmer = PorterStemmer()\n",
    "# Define a consistent max_length for tokenization\n",
    "max_length = 128\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(t5_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the custom Seq2SeqDataset class (if needed)\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input_length=max_length, max_target_length=max_length):\n",
    "        self.data = data  # The dataset containing the text examples\n",
    "        self.tokenizer = tokenizer  # The tokenizer for converting text to tokens\n",
    "        self.max_input_length = max_input_length  # Max length for input sequences\n",
    "        self.max_target_length = max_target_length  # Max length for target sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)  # Returns the total number of examples in the dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Fetch the original and rephrased questions for the given index\n",
    "        input_text = self.data['original_question'][index]\n",
    "        target_text = self.data['rephrased_question'][index]\n",
    "\n",
    "        # Tokenize the input and target texts\n",
    "        input_ids = self.tokenizer(input_text, padding='max_length', max_length=self.max_input_length, truncation=True, return_tensors='pt').input_ids\n",
    "        target_ids = self.tokenizer(target_text, padding='max_length', max_length=self.max_target_length, truncation=True, return_tensors='pt').input_ids\n",
    "\n",
    "        # Return a dictionary with input and target token IDs\n",
    "        return {'input_ids': input_ids.squeeze(), 'labels': target_ids.squeeze()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2. Load and split the dataset\n",
    "# def load_and_split_dataset(dataset_path, split_ratios=(0.8, 0.1, 0.1)):\n",
    "#     # Load the full dataset from the specified path\n",
    "#     dataset = load_from_disk(dataset_path)\n",
    "\n",
    "#     # Split dataset into training and test sets first\n",
    "#     split_dataset = dataset.train_test_split(test_size=split_ratios[2])\n",
    "\n",
    "#     # Further split the training set into train and validation sets\n",
    "#     temp_dataset = split_dataset['train'].train_test_split(test_size=split_ratios[1] / (split_ratios[0] + split_ratios[1]))\n",
    "\n",
    "#     # Combine the splits into a final DatasetDict\n",
    "#     final_splits = DatasetDict({\n",
    "#         'train': temp_dataset['train'],\n",
    "#         'validation': temp_dataset['test'],\n",
    "#         'test': split_dataset['test']\n",
    "#     })\n",
    "\n",
    "#     # Return the split datasets\n",
    "#     return final_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_dataset(dataset_path, split_ratios=(0.8, 0.1, 0.1)):\n",
    "    # Load the dataset from the specified path\n",
    "    dataset = load_from_disk(dataset_path)\n",
    "    \n",
    "    # Check if the dataset already has 'train', 'validation', and 'test' keys\n",
    "    if all(key in dataset.keys() for key in ['train', 'validation', 'test']):\n",
    "        print(\"Dataset already contains 'train', 'validation', and 'test' splits.\")\n",
    "        return dataset  # Return the existing splits\n",
    "    \n",
    "    # If not, perform the splits manually\n",
    "    print(\"Splitting the dataset into 'train', 'validation', and 'test' sets.\")\n",
    "    \n",
    "    # Split dataset into training and test sets first\n",
    "    split_dataset = dataset.train_test_split(test_size=split_ratios[2])\n",
    "\n",
    "    # Further split the training set into train and validation sets\n",
    "    temp_dataset = split_dataset['train'].train_test_split(test_size=split_ratios[1] / (split_ratios[0] + split_ratios[1]))\n",
    "\n",
    "    # Combine the splits into a final DatasetDict\n",
    "    final_splits = DatasetDict({\n",
    "        'train': temp_dataset['train'],\n",
    "        'validation': temp_dataset['test'],\n",
    "        'test': split_dataset['test']\n",
    "    })\n",
    "\n",
    "    # Return the split datasets\n",
    "    return final_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure consistent tokenization settings across the script\n",
    "def tokenize_input(texts):\n",
    "    return tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Preprocess the data using tokenizer\n",
    "def preprocess_data(tokenizer, dataset, max_input_length=max_length, max_target_length=max_length):\n",
    "    # This function tokenizes the input and target sequences in the dataset\n",
    "    def tokenize_function(examples):\n",
    "        # Tokenize the original question\n",
    "        model_inputs = tokenizer(\n",
    "            examples[\"original_question\"], \n",
    "            max_length=max_input_length, \n",
    "            truncation=True, \n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "        # Tokenize the rephrased question and store it as labels\n",
    "        labels = tokenizer(\n",
    "            examples[\"rephrased_question\"], \n",
    "            max_length=max_target_length, \n",
    "            truncation=True, \n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "    # Apply the tokenization function to the dataset\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text normalization function\n",
    "def normalize_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    # Remove extra whitespaces (e.g., multiple spaces) and trim leading/trailing spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Apply stemming to reduce words to their root form\n",
    "    text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "    \n",
    "    # Return the normalized text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute normalized edit distance\n",
    "def compute_normalized_edit_distance(pred, label):\n",
    "    # Calculate the raw Edit Distance\n",
    "    raw_edit_distance = nltk.edit_distance(pred, label)\n",
    "    \n",
    "    # Normalize by the length of the longer string\n",
    "    normalized_edit_distance = raw_edit_distance / max(len(pred), len(label))\n",
    "    \n",
    "    return normalized_edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define compute_metrics function with proper handling of text-based and embedding-based metrics\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions.argmax(-1)\n",
    "    \n",
    "    # Load the tokenizer to decode the prediction and label IDs back to text\n",
    "    # tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "    \n",
    "    # Decode the predictions and labels into strings\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Normalize the decoded strings\n",
    "    pred_str_norm = [normalize_text(text) for text in pred_str]\n",
    "    labels_str_norm = [normalize_text(text) for text in labels_str]\n",
    "    \n",
    "    # Compute text-based metrics\n",
    "    # bleu_score = np.mean([sentence_bleu([label], pred) for label, pred in zip(labels_str_norm, pred_str_norm)])\n",
    "    \n",
    "    # rouge_scorer_obj = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    # rouge_scores = [rouge_scorer_obj.score(label, pred) for label, pred in zip(labels_str_norm, pred_str_norm)]\n",
    "    # avg_rouge = {metric: np.mean([score[metric].fmeasure for score in rouge_scores]) for metric in rouge_scores[0].keys()}\n",
    "    \n",
    " \n",
    "    # meteor_scores = meteor.compute(predictions=pred_str_norm, references=labels_str_norm)\n",
    "    \n",
    "    # edit_distance = np.mean([nltk.edit_distance(pred, label) for pred, label in zip(pred_str_norm, labels_str_norm)])\n",
    "\n",
    "    # Normalized Edit Distance\n",
    "    # normalized_edit_distance = np.mean([compute_normalized_edit_distance(pred, label) for pred, label in zip(pred_str_norm, labels_str_norm)])\n",
    "\n",
    "    # Compute embedding-based metrics\n",
    "    # Tokenize for embedding-based metrics\n",
    "    # pred_embeddings = tokenize_input(pred_str)['input_ids']\n",
    "    # labels_embeddings = tokenize_input(labels_str)['input_ids']\n",
    "\n",
    "    # Get embeddings for predicted and reference strings using consistent tokenization\n",
    "    pred_embeddings = tokenizer(pred_str, padding=True, truncation=True, max_length=max_length, return_tensors='pt')['input_ids']\n",
    "    labels_embeddings = tokenizer(labels_str, padding=True, truncation=True, max_length=max_length, return_tensors='pt')['input_ids']\n",
    "\n",
    "    # Move embeddings to GPU if available\n",
    "    # if torch.cuda.is_available():\n",
    "    #     pred_embeddings = pred_embeddings.cuda()\n",
    "    #     labels_embeddings = labels_embeddings.cuda()\n",
    "\n",
    "    # Cosine similarity between embeddings\n",
    "    # cosine_sim_scores = [1 - cosine(pred_embedding.cpu(), label_embedding.cpu()) for pred_embedding, label_embedding in zip(pred_embeddings, labels_embeddings)]\n",
    "    # avg_cosine_similarity = np.mean(cosine_sim_scores)\n",
    "\n",
    "    # Compute cosine similarity directly on the GPU using PyTorch\n",
    "    cosine_sim_scores = F.cosine_similarity(pred_embeddings.float(), labels_embeddings.float(), dim=-1)\n",
    "    avg_cosine_similarity = torch.mean(cosine_sim_scores).item()\n",
    "    \n",
    "    # Perplexity (logits-based, not embedding-based)\n",
    "    # perplexity = torch.exp(torch.mean(pred.predictions))\n",
    "\n",
    "    # Return the computed metrics as a dictionary\n",
    "    return {\n",
    "        'cosine_similarity': avg_cosine_similarity,\n",
    "        # 'bleu': bleu_score,\n",
    "        # 'rouge1': avg_rouge['rouge1'],\n",
    "        # 'rougeL': avg_rouge['rougeL'],\n",
    "        # 'meteor': meteor_scores['meteor'],\n",
    "        # 'perplexity': perplexity.item(),\n",
    "        # 'edit_distance': edit_distance,\n",
    "        # 'normalized_edit_distance': normalized_edit_distance\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Set Up the Model and Training Arguments\n",
    "def setup_model_and_args(model_name=\"t5-small\", output_dir=\"./results\", **kwargs):\n",
    "    # Load the T5 model and tokenizer\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Move model to GPU if available\n",
    "    # if torch.cuda.is_available():\n",
    "    #     model.cuda()\n",
    "    # Default training arguments in a dictionary\n",
    "    training_args_dict = {\n",
    "        \"output_dir\":output_dir,  # Output directory for model checkpoints\n",
    "        \"eval_strategy\":'steps',  # Evaluate model at the end of each epoch\n",
    "        \"save_strategy\":'steps',  # Save the model every few steps\n",
    "        \"learning_rate\":5e-5,  # Learning rate for optimizer\n",
    "        \"warmup_steps\":300,\n",
    "        \"per_device_train_batch_size\":16,  # Batch size for training\n",
    "        \"per_device_eval_batch_size\":16,  # Batch size for evaluation\n",
    "        \"weight_decay\":0.01,  # Weight decay to avoid overfitting\n",
    "        \"num_train_epochs\":1,  # Number of training epochs\n",
    "        \"fp16\":False,#torch.cuda.is_available(),  # Enable mixed precision training if a GPU is available\n",
    "        \"logging_dir\":'./logs',  # Directory for storing logs\n",
    "        \"logging_steps\":50,  # Log training progress every 10 steps\n",
    "        \"eval_steps\":50,  # Evaluate the model every 500 steps\n",
    "        \"save_steps\":100,  # Save model checkpoint every 500 steps\n",
    "        \"load_best_model_at_end\":True,  # Load the best model found during training\n",
    "        # \"metric_for_best_model\": \"cosine_similarity\",  # Example: Monitor BLEU score to save the best model\n",
    "        # \"greater_is_better\": True,  # Set to False if a lower value is better (e.g., for loss)\n",
    "        \"metric_for_best_model\": \"eval_loss\",\n",
    "        \"greater_is_better\": False,\n",
    "    }\n",
    "\n",
    "    # Update the dictionary with any provided kwargs\n",
    "    training_args_dict.update(kwargs)\n",
    "\n",
    "    # Pass the updated dictionary to TrainingArguments\n",
    "    training_args = TrainingArguments(**training_args_dict)\n",
    "    \n",
    "    # Return the model, tokenizer, and training arguments\n",
    "    return model, tokenizer, training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Initialize Trainer and Data Collator\n",
    "def initialize_trainer(model, tokenizer, tokenized_dataset, training_args):\n",
    "    # Define the data collator to dynamically pad the sequences during batching\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "    \n",
    "    # Initialize the Trainer class with the model, arguments, datasets, tokenizer, and data collator\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],  # Training dataset\n",
    "        eval_dataset=tokenized_dataset[\"validation\"],  # Validation dataset\n",
    "        tokenizer=tokenizer,  # Tokenizer to decode the predictions\n",
    "        data_collator=data_collator,  # Data collator for padding\n",
    "        # compute_metrics=compute_metrics  # Function to compute evaluation metrics\n",
    "    )\n",
    "    \n",
    "    # Return the initialized Trainer\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Train the Model and Evaluate\n",
    "def train_and_evaluate_model(trainer):\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "    return eval_results\n",
    "\n",
    "# 8. Save the Model and Tokenizer\n",
    "def save_model_and_tokenizer(trainer, tokenizer, output_dir=\"./fine_tuned_t5\"):\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Plot training and validation losses\n",
    "def plot_train_validation_losses(trainer):\n",
    "    train_loss = trainer.state.log_history\n",
    "    steps = range(len(train_loss))\n",
    "    train_losses = [x['loss'] for x in train_loss if 'loss' in x]\n",
    "    eval_losses = [x['eval_loss'] for x in train_loss if 'eval_loss' in x]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Training loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(steps[:len(train_losses)], train_losses, label='Training Loss')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Validation loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(steps[:len(eval_losses)], eval_losses, label='Validation Loss', color='orange')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Validation Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 10. Plot evaluation metrics over iterations\n",
    "def plot_evaluation_metrics(trainer):\n",
    "    log_history = trainer.state.log_history\n",
    "\n",
    "    # Separate metrics\n",
    "    eval_steps = [x['step'] for x in log_history if 'eval_loss' in x]\n",
    "    bleu_scores = [x['eval_bleu'] for x in log_history if 'eval_bleu' in x]\n",
    "    rouge1_scores = [x['eval_rouge1'] for x in log_history if 'eval_rouge1' in x]\n",
    "    rougeL_scores = [x['eval_rougeL'] for x in log_history if 'eval_rougeL' in x]\n",
    "    meteor_scores = [x['eval_meteor'] for x in log_history if 'eval_meteor' in x]\n",
    "    cosine_similarity_scores = [x['eval_cosine_similarity'] for x in log_history if 'eval_cosine_similarity' in x]\n",
    "    perplexity_scores = [x['eval_perplexity'] for x in log_history if 'eval_perplexity' in x]\n",
    "    edit_distances = [x['eval_edit_distance'] for x in log_history if 'eval_edit_distance' in x]\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "\n",
    "    # Subplot 1: Evaluation metrics except Edit Distance\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(eval_steps, bleu_scores, label='BLEU', color='blue')\n",
    "    plt.plot(eval_steps, rouge1_scores, label='ROUGE-1', color='green')\n",
    "    plt.plot(eval_steps, rougeL_scores, label='ROUGE-L', color='red')\n",
    "    plt.plot(eval_steps, meteor_scores, label='METEOR', color='purple')\n",
    "    plt.plot(eval_steps, cosine_similarity_scores, label='Cosine Similarity', color='brown')\n",
    "    plt.plot(eval_steps, perplexity_scores, label='Perplexity', color='magenta')\n",
    "    plt.xlabel('Evaluation Steps')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Evaluation Metrics Over Iterations (Excluding Edit Distance)')\n",
    "    plt.legend()\n",
    "\n",
    "    # Subplot 2: Edit Distance\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(eval_steps, edit_distances, label='Edit Distance', color='orange')\n",
    "    plt.xlabel('Evaluation Steps')\n",
    "    plt.ylabel('Edit Distance')\n",
    "    plt.title('Edit Distance Over Iterations')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to orchestrate the fine-tuning process\n",
    "def fine_tune_t5(dataset_path, output_dir=\"./fine_tuned_t5\",model_name='t5_small',split_ratios=(0.8, 0.1, 0.1), **training_args):\n",
    "    dataset_splits = load_and_split_dataset(dataset_path, split_ratios)\n",
    "    model, tokenizer, training_args = setup_model_and_args(\n",
    "        model_name=model_name,\n",
    "        output_dir=output_dir, \n",
    "        **training_args\n",
    "    )\n",
    "    tokenized_dataset = preprocess_data(tokenizer, dataset_splits)\n",
    "    trainer = initialize_trainer(model, tokenizer, tokenized_dataset, training_args)\n",
    "\n",
    "    # Train and Evaluate\n",
    "    eval_results = train_and_evaluate_model(trainer)\n",
    "    print(f\"Evaluation results: {eval_results}\")\n",
    "\n",
    "    # Save Model and Tokenizer\n",
    "    save_model_and_tokenizer(trainer, tokenizer, output_dir)\n",
    "\n",
    "    # Plot losses\n",
    "    # plot_train_validation_losses(trainer)\n",
    "\n",
    "    # Plot evaluation metrics\n",
    "    # plot_evaluation_metrics(trainer)\n",
    "\n",
    "    return trainer, eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot training and evaluation metrics after training\n",
    "def plot_training_and_evaluation_results(trainer):\n",
    "    # Plot training and validation losses\n",
    "    plot_train_validation_losses(trainer)\n",
    "\n",
    "    # Plot evaluation metrics\n",
    "    plot_evaluation_metrics(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already contains 'train', 'validation', and 'test' splits.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 50/28160 [00:55<8:29:30,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 11.9363, 'grad_norm': 50.36502456665039, 'learning_rate': 4.9999999999999996e-06, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# trainer, results = fine_tune_t5(\"msmarco_dataset\", output_dir=\"./fine_tuned_t5_masmarco\", num_train_epochs=1, split_ratios=(0.8, 0.1, 0.1),\n",
    "#                  learning_rate=3e-5, per_device_train_batch_size=32,per_device_eval_batch_size=32)\n",
    "\n",
    "trainer, results = fine_tune_t5(\n",
    "        \"hotpotqa_dataset\", \n",
    "        output_dir=\"./fine_tuned_t5_hotpotqa\", \n",
    "        model_name = \"t5-small\",\n",
    "        split_ratios=(0.8, 0.1, 0.1),\n",
    "        num_train_epochs=5, \n",
    "        use_cpu = False,\n",
    "        learning_rate=3e-5,  # Override the default learning rate\n",
    "        per_device_train_batch_size=16, \n",
    "        per_device_eval_batch_size=4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_training_and_evaluation_results(trainer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
